{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c7e803bcb692553a80f9fdded4c1a4f9a7897428"
   },
   "source": [
    "# MLDM Lab week 5: SVM and Clustering and application in hand-written digit recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> <font color=\"blue\"> Introduction </h3>\n",
    "\n",
    "In this lab session, we explore a new dataset on hand-written digits and use SVM and clustering algorithms for hand-written digit recognition.\n",
    "    \n",
    "We also revisit the Iris and the Breast Cancer datasets from previous lab sessions and train new classifiers and compare the results with previous classifiers. Please see the information regarding these datasets / classifiers from previous lab sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da9c6cd47b5c79bed433cc47618a56837322db5d"
   },
   "source": [
    "<h3> <font color=\"blue\"> Lab goals</font> </h3>\n",
    "<p> 1.  Exploring hand-written digit recognition dataset. </p>\n",
    "<p> 2.  Learn how to use SVM and clustering algorithms from relevant `sklearn` libraries. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Hand-written digit recognition dataset\n",
    "In this section we explore the hand-written digit recognition dataset from `sklearn`.\n",
    "    \n",
    "This dataset is made up of 1797 8x8 images. Each image, like the ones shown below, is of a hand-written digit. In order to utilize an 8x8 figure like this, we'd have to first transform it into a feature vector with length 64. See here\n",
    "<https://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits>_ for more information about this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load the digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    # label the image with the target value\n",
    "    ax.text(0, 7, str(digits.target[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Hand-written digit recognition using SVM\n",
    "In this section we train an SVM classifier for the task of hand-written digit recognition using the dataset described above.\n",
    "    \n",
    "We use an implementation of SVM from `sklearn`. More information on this implementation can be found <a href=\"https://scikit-learn.org/stable/modules/svm.html\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# The data that we are interested in is made of 8x8 images of digits, let's\n",
    "# have a look at the first 4 images, stored in the `images` attribute of the\n",
    "# dataset.  If we were working from image files, we could load them using\n",
    "# matplotlib.pyplot.imread.  Note that each image must have the same size. For these\n",
    "# images, we know which digit they represent: it is given in the 'target' of\n",
    "# the dataset.\n",
    "_, axes = plt.subplots(2, 4)\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for ax, (image, label) in zip(axes[0, :], images_and_labels[:4]):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Training: %i' % label)\n",
    "\n",
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "classifier = svm.SVC(gamma=0.001)\n",
    "\n",
    "# Split data into train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, digits.target, test_size=0.5, shuffle=False)\n",
    "\n",
    "# We learn the digits on the first half of the digits\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\n",
    "for ax, (image, prediction) in zip(axes[1, :], images_and_predictions[:4]):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Prediction: %i' % prediction)\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(y_test, predicted)))\n",
    "disp = metrics.plot_confusion_matrix(classifier, X_test, y_test)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(\"Confusion matrix:\\n%s\" % disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"red\">Exercise 1 </font> </h3>\n",
    "<p> <p>Repeat the experiment above and compare the performance (accuracy) of SVM with a Naive Bayes classifier on the digits dataset (you can use the codes from last week)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Exercise 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Hand-written digit recognition using PCA and K-means\n",
    "    \n",
    "In this section we use a combination of PCA (to reduce the data) and K-means (for clustering) for the task of hand-written digit recognition using the dataset described above. We also plot a Voronoi Diagram to visualise the clusters.\n",
    "    \n",
    "We use the implementations of PCA and K-means from `sklearn.decomposition` and `sklearn.cluster`. More information about the PCA implementation can be found <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">here</a>  and more information about the K-means implementation can be found <a href=\"https://scikit-learn.org/stable/modules/clustering.html#k-means\">here</a>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X_digits, y_digits = load_digits(return_X_y=True)\n",
    "data = scale(X_digits)\n",
    "\n",
    "n_samples, n_features = data.shape\n",
    "n_digits = len(np.unique(y_digits))\n",
    "labels = y_digits\n",
    "\n",
    "sample_size = 300\n",
    "\n",
    "print(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\"\n",
    "      % (n_digits, n_samples, n_features))\n",
    "\n",
    "print('\\n-------------- Metrics before PCA -------------------')\n",
    "print(82 * '_')\n",
    "print('init\\t\\tfeat\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette')\n",
    "\n",
    "\n",
    "def bench_k_means(estimator, name, data):\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    \n",
    "        \n",
    "    print('%-9s\\t%i\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "          % (name, data.shape[1],(time() - t0), estimator.inertia_,\n",
    "             metrics.homogeneity_score(labels, estimator.labels_),\n",
    "             metrics.completeness_score(labels, estimator.labels_),\n",
    "             metrics.v_measure_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "             metrics.silhouette_score(data, estimator.labels_,\n",
    "                                      metric='euclidean',\n",
    "                                      sample_size=sample_size)))\n",
    "    \n",
    "bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n",
    "              name=\"k-means++\", data=data)\n",
    "\n",
    "#bench_k_means(KMeans(init='random init.', n_clusters=n_digits, n_init=10),\n",
    "#              name=\"random init.\", data=data)\n",
    "\n",
    "#\n",
    "print(82 * '_')\n",
    "\n",
    "print('\\n-------------- Metrics after PCA with r=24 -------------------')\n",
    "print(82 * '_')\n",
    "print('init\\t\\tfeat\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette')\n",
    "\n",
    "    \n",
    "reduced_data0 = PCA(n_components=24).fit_transform(data)\n",
    "kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
    "kmeans.fit(reduced_data0)\n",
    "\n",
    "\n",
    "bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n",
    "              name=\"k-means++\", data=reduced_data0)\n",
    "\n",
    "print(82 * '_')\n",
    "\n",
    "print('\\n-------------- Metrics after PCA with r=2 -------------------')\n",
    "print(82 * '_')\n",
    "print('init\\t\\tfeat\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette')\n",
    "\n",
    "    \n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
    "kmeans.fit(reduced_data)\n",
    "\n",
    "\n",
    "bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n",
    "              name=\"k-means++\", data=reduced_data)\n",
    "\n",
    "#bench_k_means(KMeans(init='random init.', n_clusters=n_digits, n_init=10),\n",
    "#              name=\"random init\", data=reduced_data)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Visualize the results on PCA-reduced data\n",
    "\n",
    "#reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "#kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
    "#kmeans.fit(reduced_data)\n",
    "\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
    "kmeans.fit(reduced_data)\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('K-means clustering on the digits dataset (PCA-reduced data)\\n'\n",
    "          'Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"red\">Exercise 2 </font> </h3>\n",
    "<p> The table above compares the the performance of K-means clustering algorithm with and without the dimension reductions using PCA (with r=24 and r=2) according to different performance evaluation metrics (homogeneity score, completeness score, V measure, adjusted rand index, adjusted mutual information, and silhouette coefficient). What is the overall effect of the dimension reductions (for 64->24 and 64->2) on this dataset and what do you think might be the reason.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"red\">Save your notebook after completing the exercises and submit it to SurreyLearn (Assessments -> Assignments -> Lab Exercises - Week 5) as a python notebook file in ipynb formt. </h3>\n",
    "<h3><font color=\"red\">Deadline: 4:00pm Thursday 14 Mar  </h3> \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
